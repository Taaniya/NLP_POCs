{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subword_algorithms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alfYyDbMG63a"
      },
      "source": [
        "### Why subword tokenization is needed?\r\n",
        "\r\n",
        "1. The understanding of model will be limited to words in the vocabulary created from training dataset. eg. words like talking, talks will not be recognised by model and will be replaced  by 'UNK' if the vocabulary only contains 'talk'. Though some cases can be dealt with stemming and lemmatization, it will still add an extra step in your NLP pipeline for preprocessing and will be limited to languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwvhNePAwVe"
      },
      "source": [
        "### Byte-pair Encoding\r\n",
        "\r\n",
        "Greedy algorithm which checks for every sequence pair's frequency in every iteration and adds the pair with highest frequency in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhCMtqHLjy1e"
      },
      "source": [
        "import re, collections"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtq53P8VtrFl",
        "outputId": "815bb062-5a3d-410f-b91b-d23c5090e9be"
      },
      "source": [
        "def get_stats(vocab):\r\n",
        "  pairs = collections.defaultdict(int)\r\n",
        "\r\n",
        "  for word, freq in vocab.items():\r\n",
        "    symbols = word.split()\r\n",
        "    for i in range(len(symbols)-1):\r\n",
        "      pairs[(symbols[i], symbols[i+1])] += freq\r\n",
        "  return pairs\r\n",
        "\r\n",
        "def merge_vocab(pair, v_in):\r\n",
        "  bigram = re.escape(' '.join(pair))\r\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')                # pattern where bigram is not preceded by & NOT followed by NON-white space characters.\r\n",
        "  v_out = {}                                                     # here, the string 'b e l o w' will satisy the pattern for bigram (l,o) \r\n",
        "  for word in v_in:                                              # since l is instead preceded by white space characters\r\n",
        "    w_out = p.sub(''.join(pair), word) \r\n",
        "    v_out[w_out] = v_in[word]\r\n",
        "  return v_out \r\n",
        "\r\n",
        "vocab = {'l o w </w>':5, 'l o w e r </w>':2, \r\n",
        "         'n e w e s t </w>':6, 'w i d e s t </w>':3}\r\n",
        "\r\n",
        "num_merges = 10\r\n",
        "for i in range(num_merges):\r\n",
        "  pairs = get_stats(vocab)\r\n",
        "  best = max(pairs, key=pairs.get)\r\n",
        "  vocab = merge_vocab(best, vocab)\r\n",
        "  print(\"best pair {}\".format(best))\r\n",
        "  print(\"vocab {}\".format(vocab))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best pair ('e', 's')\n",
            "vocab {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "best pair ('es', 't')\n",
            "vocab {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "best pair ('est', '</w>')\n",
            "vocab {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('l', 'o')\n",
            "vocab {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('lo', 'w')\n",
            "vocab {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('n', 'e')\n",
            "vocab {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('ne', 'w')\n",
            "vocab {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('new', 'est</w>')\n",
            "vocab {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('low', '</w>')\n",
            "vocab {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "best pair ('w', 'i')\n",
            "vocab {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnog_kfwA4l4"
      },
      "source": [
        "#### Limitations \r\n",
        "* Ambiguous token vocabulary - \r\n",
        "A word can be broken into subword in multiple ways.And if the subwords for all multiple ways are present in the vocabulary it brings ambiguity in terms of deciding in what way to break a new word down into its constituent subwords?\r\n",
        "Since it doesn't prioritize which subwords to use first.\r\n",
        "\r\n",
        "\r\n",
        "### References\r\n",
        "\r\n",
        "* https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46\r\n",
        "* https://blog.floydhub.com/tokenization-nlp/\r\n",
        "* https://arxiv.org/pdf/1804.10959.pdf"
      ]
    }
  ]
}